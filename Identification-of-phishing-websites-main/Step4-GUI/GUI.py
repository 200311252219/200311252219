import streamlit as st
import pandas as pd
import joblib
import os
import urllib.parse
import socket
import tldextract
import datetime
import requests
import whois
import warnings
import sqlite3
from bs4 import BeautifulSoup

# å¿½ç•¥ SSL è­¦å‘Š
warnings.filterwarnings("ignore")

# ======================================================
# 1. è·¯å¾„é…ç½®
# ======================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "..", "Step3-Modeling")
DB_PATH = os.path.join(BASE_DIR, "phishing_system.db")

MODEL_PATH = os.path.join(MODEL_DIR, "phishing_model.pkl")
SCALER_PATH = os.path.join(MODEL_DIR, "feature_scaler.pkl")
SELECTOR_PATH = os.path.join(MODEL_DIR, "feature_selector.pkl")
COLUMNS_PATH = os.path.join(MODEL_DIR, "feature_columns.pkl")

# ======================================================
# 2. ç•Œé¢é…ç½® & éšè—é»˜è®¤èœå•
# ======================================================
st.set_page_config(
    page_title="æ™ºèƒ½é’“é±¼ç½‘ç«™æ£€æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': "### ğŸ›¡ï¸ æ™ºèƒ½é’“é±¼ç½‘ç«™æ£€æµ‹ç³»ç»Ÿ v1.0\n\næœ¬ç³»ç»ŸåŸºäº **éšæœºæ£®æ— (Random Forest)** ç®—æ³•æ„å»ºï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·è¯†åˆ«æ¶æ„é’“é±¼é“¾æ¥ã€‚"
    }
)

# --- éšè— Streamlit é»˜è®¤é£æ ¼ (Deploy æŒ‰é’®ç­‰) ---
hide_streamlit_style = """
<style>
    .stDeployButton {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)


# ======================================================
# 3. æ•°æ®åº“ç®¡ç†æ¨¡å— (SQLite)
# ======================================================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS whitelist
                 (
                     id
                     INTEGER
                     PRIMARY
                     KEY
                     AUTOINCREMENT,
                     domain
                     TEXT
                     UNIQUE,
                     added_at
                     TIMESTAMP
                     DEFAULT
                     CURRENT_TIMESTAMP
                 )''')
    c.execute('''CREATE TABLE IF NOT EXISTS history
                 (
                     id
                     INTEGER
                     PRIMARY
                     KEY
                     AUTOINCREMENT,
                     url
                     TEXT,
                     result
                     TEXT,
                     probability
                     REAL,
                     timestamp
                     TIMESTAMP
                     DEFAULT
                     CURRENT_TIMESTAMP
                 )''')

    # é¢„åˆ¶ç™½åå•
    initial_whitelist = ["bilibili.com", "baidu.com", "qq.com", "google.com", "taobao.com", "jd.com"]
    for domain in initial_whitelist:
        try:
            c.execute("INSERT INTO whitelist (domain) VALUES (?)", (domain,))
        except sqlite3.IntegrityError:
            pass
    conn.commit()
    conn.close()


def add_to_whitelist(domain):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute("INSERT INTO whitelist (domain) VALUES (?)", (domain,))
        conn.commit()
        return True
    except sqlite3.IntegrityError:
        return False
    finally:
        conn.close()


def get_whitelist():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT domain as 'åŸŸå', added_at as 'æ·»åŠ æ—¶é—´' FROM whitelist ORDER BY added_at DESC",
                           conn)
    conn.close()
    return df


def save_history(url, result, probability):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("INSERT INTO history (url, result, probability) VALUES (?, ?, ?)",
              (url, result, probability))
    conn.commit()
    conn.close()


def get_history():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT url, result, probability, timestamp FROM history ORDER BY timestamp DESC LIMIT 50",
                           conn)
    conn.close()
    # é‡å‘½ååˆ—ä»¥ä¾¿å±•ç¤º
    df.columns = ["æ£€æµ‹é“¾æ¥", "æ£€æµ‹ç»“æœ", "ç½®ä¿¡åº¦", "æ£€æµ‹æ—¶é—´"]
    return df


if not os.path.exists(DB_PATH):
    init_db()


# ======================================================
# 4. æ ¸å¿ƒåŠ è½½å‡½æ•°
# ======================================================
@st.cache_resource
def load_resources():
    try:
        model = joblib.load(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        selector = joblib.load(SELECTOR_PATH)

        if os.path.exists(COLUMNS_PATH):
            feature_cols = joblib.load(COLUMNS_PATH)
        else:
            return None, None, None, None
        return model, scaler, selector, feature_cols
    except FileNotFoundError:
        return None, None, None, None


model, scaler, selector, feature_columns = load_resources()


# ======================================================
# 5. ä¸šåŠ¡é€»è¾‘å‡½æ•°
# ======================================================
def check_whitelist_db(url):
    try:
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}".lower()
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        c.execute("SELECT 1 FROM whitelist WHERE domain = ?", (domain,))
        result = c.fetchone()
        conn.close()
        if result:
            return True, domain
    except:
        pass
    return False, None


# --- ç‰¹å¾æå– (ä¿æŒé€»è¾‘ä¸å˜) ---
def extract_address_bar_features(url):
    features = {}
    features["url_length"] = len(url)
    features["num_dots"] = url.count(".")
    features["protocol"] = 1 if urllib.parse.urlparse(url).scheme == "https" else 0
    try:
        domain = urllib.parse.urlparse(url).netloc
        socket.inet_aton(domain.split(":")[0])
        features["uses_ip"] = 1
    except:
        features["uses_ip"] = 0
    try:
        ext = tldextract.extract(url)
        features["num_subdomains"] = len(ext.subdomain.split(".")) if ext.subdomain else 0
    except:
        features["num_subdomains"] = 0
    return features


def extract_domain_features(url):
    features = {"domain_age_days": -1, "dns_valid": 0, "whois_info_exists": 0}
    try:
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        try:
            w = whois.whois(domain)
            if w.creation_date:
                c = w.creation_date
                if isinstance(c, list): c = c[0]
                if isinstance(c, datetime.datetime):
                    features["domain_age_days"] = (datetime.datetime.now() - c).days
            features["whois_info_exists"] = 1 if w else 0
        except:
            pass
        try:
            socket.gethostbyname(domain)
            features["dns_valid"] = 1
        except:
            pass
    except:
        pass
    return features


def extract_html_features(url):
    features = {"has_iframe": -1, "has_obfuscated_js": -1}
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        r = requests.get(url, headers=headers, timeout=5, verify=False)
        soup = BeautifulSoup(r.text, "html.parser")
        features["has_iframe"] = 1 if soup.find("iframe") else 0
        scripts = soup.find_all("script")
        for s in scripts:
            if s.string:
                txt = s.string.lower()
                if "eval(" in txt and len(txt) < 1000:
                    features["has_obfuscated_js"] = 1;
                    break
                if "\\x" in txt and len(txt) > 500:
                    features["has_obfuscated_js"] = 1;
                    break
        else:
            features["has_obfuscated_js"] = 0
    except:
        pass
    return features


def extract_features_pipeline(url, required_columns):
    f = {}
    f.update(extract_address_bar_features(url))
    f.update(extract_domain_features(url))
    f.update(extract_html_features(url))
    df = pd.DataFrame([f])
    df_aligned = pd.DataFrame(columns=required_columns)
    for col in required_columns:
        if col in df.columns:
            df_aligned.loc[0, col] = df.loc[0, col]
        else:
            df_aligned.loc[0, col] = 0
    return df_aligned


# ======================================================
# 6. ä¸»ç•Œé¢é€»è¾‘ (å…¨ä¸­æ–‡)
# ======================================================

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("âš™ï¸ ç®¡ç†åå°")

    tab1, tab2 = st.tabs(["ğŸ“ å†å²è®°å½•", "ğŸ›¡ï¸ ç™½åå•ç®¡ç†"])

    with tab1:
        st.caption("æœ€è¿‘ 50 æ¡æ£€æµ‹è®°å½•")
        if st.button("ğŸ”„ åˆ·æ–°è®°å½•", use_container_width=True):
            pass
        history_df = get_history()
        st.dataframe(history_df, use_container_width=True, hide_index=True)

    with tab2:
        st.subheader("æ·»åŠ ä¿¡ä»»åŸŸå")
        new_domain = st.text_input("è¾“å…¥åŸŸå (å¦‚ jd.com)", key="new_domain")
        if st.button("â• æ·»åŠ è‡³ç™½åå•", use_container_width=True):
            if new_domain:
                if add_to_whitelist(new_domain):
                    st.success(f"å·²æˆåŠŸæ·»åŠ : {new_domain}")
                else:
                    st.warning("æ·»åŠ å¤±è´¥ï¼šåŸŸåå·²å­˜åœ¨æˆ–æ ¼å¼æ— æ•ˆ")

        st.divider()
        st.subheader("å½“å‰ç™½åå•åˆ—è¡¨")
        whitelist_df = get_whitelist()
        st.dataframe(whitelist_df, use_container_width=True, hide_index=True)

# --- ä¸»å†…å®¹åŒº ---
st.title("ğŸ›¡ï¸ æ™ºèƒ½é’“é±¼ç½‘ç«™æ£€æµ‹ç³»ç»Ÿ")
st.markdown("### åŸºäºæœºå™¨å­¦ä¹  (Machine Learning) çš„å®æ—¶å¨èƒæ£€æµ‹å¹³å°")
st.markdown("---")

col1, col2 = st.columns([3, 1])
with col1:
    url_input = st.text_input("ğŸ”— è¯·è¾“å…¥ç›®æ ‡ URLï¼š", placeholder="ä¾‹å¦‚ï¼šhttps://www.example.com")
with col2:
    st.write("")
    st.write("")
    check_btn = st.button("ğŸš€ å¼€å§‹æ£€æµ‹", type="primary", use_container_width=True)

if check_btn:
    if not url_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„ URL é“¾æ¥")

    elif model is None:
        st.error("âŒ ç³»ç»Ÿæ•…éšœï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ Step3-Modeling ç›®å½•æ˜¯å¦å®Œæ•´ã€‚")

    else:
        # --- æ­¥éª¤ 1: æ•°æ®åº“ç™½åå•æ£€æŸ¥ ---
        is_safe, domain_name = check_whitelist_db(url_input)

        if is_safe:
            st.balloons()
            st.success(f"âœ… **æ£€æµ‹ç»“æœï¼šå®‰å…¨ (Safe)**")
            st.info(f"åŸŸå `{domain_name}` ä½äºç³»ç»Ÿçš„ä¿¡ä»»ç™½åå•ä¸­ï¼Œæ— éœ€è¿›è¡Œæ¨¡å‹è¿ç®—ã€‚")

            # è®°å½•å†å² (ä¸­æ–‡)
            save_history(url_input, "å®‰å…¨ (ç™½åå•)", 1.0)

            with st.expander("ğŸ” æŸ¥çœ‹ç‰¹å¾è¯¦æƒ… (åå°æå–)"):
                with st.spinner("æ­£åœ¨æå–ç‰¹å¾..."):
                    df_raw = extract_features_pipeline(url_input, feature_columns)
                    st.dataframe(df_raw)

        # --- æ­¥éª¤ 2: æ¨¡å‹é¢„æµ‹ ---
        else:
            with st.spinner("ğŸ” æ­£åœ¨è¿›è¡Œæ·±åº¦æ£€æµ‹ (ç‰¹å¾æå– -> æ™ºèƒ½åˆ†æ)..."):
                try:
                    # é¢„æµ‹æµç¨‹
                    X_input = extract_features_pipeline(url_input, feature_columns)
                    X_scaled = scaler.transform(X_input)
                    X_selected = selector.transform(X_scaled)

                    prediction = model.predict(X_selected)[0]
                    proba = model.predict_proba(X_selected)[0]

                    # ç»“æœæ–‡æ¡ˆå¤„ç†
                    if prediction == 1:
                        result_text = "é’“é±¼ç½‘ç«™"
                        result_prob = proba[1]
                    else:
                        result_text = "æ­£å¸¸ç½‘ç«™"
                        result_prob = proba[0]

                    # å­˜å…¥æ•°æ®åº“
                    save_history(url_input, result_text, float(result_prob))

                    # ç•Œé¢æ˜¾ç¤º
                    st.divider()
                    if prediction == 1:
                        st.error(f"ğŸš« **æ£€æµ‹ç»“æœï¼šé’“é±¼ç½‘ç«™ (Phishing)**")
                        st.metric(label="é£é™©æ¦‚ç‡", value=f"{result_prob * 100:.2f}%", delta="é«˜é£é™©")
                        st.markdown(
                            "âš ï¸ **è­¦å‘Š**ï¼šè¯¥ç½‘ç«™å‘½ä¸­å¤šä¸ªæ¶æ„ç‰¹å¾ï¼ˆå¦‚æ··æ·†ä»£ç ã€åŸŸåå¼‚å¸¸ç­‰ï¼‰ï¼Œç³»ç»Ÿåˆ¤å®šä¸ºé«˜é£é™©ï¼è¯·å‹¿è¾“å…¥ä»»ä½•ä¸ªäººä¿¡æ¯ã€‚")
                    else:
                        st.success(f"âœ… **æ£€æµ‹ç»“æœï¼šæ­£å¸¸ç½‘ç«™ (Legitimate)**")
                        st.metric(label="å®‰å…¨æ¦‚ç‡", value=f"{result_prob * 100:.2f}%", delta="å®‰å…¨")

                    # ç‰¹å¾å±•ç¤º
                    with st.expander("ğŸ”¬ æŠ€æœ¯åˆ†ææŠ¥å‘Š"):
                        st.markdown("**æ¨¡å‹è¾“å…¥ç‰¹å¾å‘é‡ (Normalized):**")
                        # ç®€å•çš„å›¾è¡¨å±•ç¤º
                        st.bar_chart(pd.DataFrame(X_selected.T, columns=["ç‰¹å¾å€¼"]))
                        st.markdown("**åŸå§‹ç‰¹å¾æ•°æ®:**")
                        st.dataframe(X_input)

                except Exception as e:
                    st.error(f"æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

st.markdown("---")
st.caption("Â© 2025 æ™ºèƒ½é’“é±¼ç½‘ç«™æ£€æµ‹ç³»ç»Ÿ | æŠ€æœ¯æ ˆï¼šPython â€¢ Scikit-Learn â€¢ Streamlit")
